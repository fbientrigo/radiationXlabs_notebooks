{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61eabff",
   "metadata": {},
   "source": [
    "# Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "236652d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "#from lib.poisson_binning import recommend_k_multiple, bin_and_rate, fit_poisson_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b567516",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_valid = pd.read_csv(\"../1_data/df2_valid.csv\")\n",
    "df3_valid = pd.read_csv(\"../1_data/df3_valid.csv\")\n",
    "\n",
    "df_beam2 = pd.read_csv(\"../1_data/beam2.csv\")\n",
    "df_beam3 = pd.read_csv(\"../1_data/beam3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fc9f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning para solo usar una porción\n",
    "df2_valid[\"failsP_acum\"] = df2_valid[[f\"bitnP{i}\" for i in range(0,32)]].sum(axis=1)\n",
    "df3_valid[\"failsP_acum\"] = df3_valid[[f\"bitnP{i}\" for i in range(0,16)]].sum(axis=1)\n",
    "\n",
    "fails_run2 = df2_valid[[\"time\", \"lfsrTMR\", \"failsP_acum\"]]\n",
    "fails_run3 = df3_valid[[\"time\", \"lfsrTMR\", \"failsP_acum\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a5fedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- run2 ---\n",
      "Beam Data:\n",
      "\tshape: (9999, 14)\n",
      "\tcolumns: Index(['Unnamed: 0', 'time', 'TID', 'HEH', 'N1MeV', 'run_group', 'dt', 'dTID',\n",
      "       'dHEH', 'dN1MeV', 'TID_dose_rate', 'N1MeV_dose_rate', 'HEH_dose_rate',\n",
      "       'beam_on'],\n",
      "      dtype='object')\n",
      "Fails Data\n",
      "\tshape: (1030123, 3)\n",
      "\tcolumns: Index(['time', 'lfsrTMR', 'failsP_acum'], dtype='object')\n",
      "--- run3 ---\n",
      "Beam Data:\n",
      "\tshape: (9997, 14)\n",
      "\tcolumns: Index(['Unnamed: 0', 'time', 'TID', 'HEH', 'N1MeV', 'run_group', 'dt', 'dTID',\n",
      "       'dHEH', 'dN1MeV', 'TID_dose_rate', 'N1MeV_dose_rate', 'HEH_dose_rate',\n",
      "       'beam_on'],\n",
      "      dtype='object')\n",
      "Fails Data\n",
      "\tshape: (1081006, 3)\n",
      "\tcolumns: Index(['time', 'lfsrTMR', 'failsP_acum'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "    print(\"--- run2 ---\")\n",
    "    print(\"Beam Data:\")\n",
    "    print(\"\\tshape:\", df_beam2.shape)\n",
    "    print(\"\\tcolumns:\",df_beam2.columns)\n",
    "    print(\"Fails Data\")\n",
    "    print(\"\\tshape:\", fails_run2.shape)\n",
    "    print(\"\\tcolumns:\",fails_run2.columns)\n",
    "\n",
    "    print(\"--- run3 ---\")\n",
    "    print(\"Beam Data:\")\n",
    "    print(\"\\tshape:\", df_beam3.shape)\n",
    "    print(\"\\tcolumns:\",df_beam3.columns)\n",
    "    print(\"Fails Data\")\n",
    "    print(\"\\tshape:\", fails_run3.shape)\n",
    "    print(\"\\tcolumns:\",fails_run3.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8a43f5",
   "metadata": {},
   "source": [
    "##  Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67198355",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Radiation reliability binning + Poisson statistics (hardened)\n",
    "\n",
    "Key features vs v1:\n",
    "- Robust time parsing (strings / datetime64 / epoch seconds)\n",
    "- Reset-aware binning, equal-fluence (scaled time) binning, equal-count binning\n",
    "- Clipped/gated scaled-time to avoid dt_eq explosions (floor + rmax cap)\n",
    "- Exact Garwood CIs (asymmetric for small-N), configurable alpha\n",
    "- Optional wall vs beam time for rate denominator\n",
    "- Area normalization (e.g., run3 16 subsystems → compare to 32)\n",
    "- Debug/QA utilities: inspectors, conservation checks, GLM trend test (optional)\n",
    "- Merge sparse bins to guarantee a minimum number of events per bin\n",
    "\n",
    "Usage sketch:\n",
    "    res = build_and_summarize(\n",
    "        df_beam2, fails_run2,\n",
    "        bin_mode=\"fluence\", n_bins=30,\n",
    "        flux_col=\"HEH_dose_rate\",\n",
    "        area_norm=(32, 32),                 # (A_run, A_ref)\n",
    "        alpha=0.32,                         # 68% CI\n",
    "        T_source=\"beam\",                   # or \"wall\"\n",
    "        min_events_per_bin=5,               # enforce stability\n",
    "    )\n",
    "\n",
    "Author: senior-dev pass\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List, Literal\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# -------------------------------\n",
    "# Time parsing\n",
    "# -------------------------------\n",
    "def to_datetime_smart(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Parse time that might be datetime64, ISO string, or epoch seconds (float/int).\n",
    "    Strategy: try generic parsing; if <90% success, fall back to numeric seconds.\n",
    "    \"\"\"\n",
    "    if np.issubdtype(s.dtype, np.datetime64):\n",
    "        return pd.to_datetime(s, errors=\"coerce\")\n",
    "    s1 = pd.to_datetime(s, errors=\"coerce\")\n",
    "    if s1.notna().mean() >= 0.9:\n",
    "        return s1\n",
    "    s_num = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return pd.to_datetime(s_num, unit=\"s\", errors=\"coerce\")\n",
    "\n",
    "# -------------------------------\n",
    "# Reset detection\n",
    "# -------------------------------\n",
    "def detect_resets(\n",
    "    fails_df: pd.DataFrame,\n",
    "    time_col: str = \"time\",\n",
    "    cum_col: str = \"failsP_acum\",\n",
    "    aux_cols: Tuple[str, ...] = (\"lfsrTMR\",),\n",
    "    min_gap_s: float = 30.0,\n",
    "    lfsr_cluster_rate_hz: float = 50.0,\n",
    ") -> List[pd.Timestamp]:\n",
    "    \"\"\"Heuristic, multi-signal reset detection.\n",
    "    Returns sorted list of boundary timestamps including first/last.\n",
    "    Signals:\n",
    "      A) Negative diff in cumulative failures (hard reset)\n",
    "      B) LFSR edge bursts (clustered edges imply reboot window)\n",
    "      C) Long inactivity gaps (plateaus)\n",
    "    \"\"\"\n",
    "    f = fails_df.sort_values(time_col).copy()\n",
    "    f[time_col] = to_datetime_smart(f[time_col]).ffill()\n",
    "\n",
    "    # A) cumulative drops\n",
    "    v = pd.to_numeric(f[cum_col], errors=\"coerce\").ffill()\n",
    "    dv = v.diff()\n",
    "    candA = f.loc[dv < 0, time_col].tolist()\n",
    "\n",
    "    # B) lfsr bursts\n",
    "    candB: List[pd.Timestamp] = []\n",
    "    for c in aux_cols:\n",
    "        if c in f.columns:\n",
    "            l = pd.to_numeric(f[c], errors=\"coerce\")\n",
    "            edges = (l != l.shift(1)) & l.notna()\n",
    "            te = f.loc[edges, time_col].astype(\"int64\") / 1e9\n",
    "            if len(te) > 1:\n",
    "                dt = np.diff(te)\n",
    "                streak = (dt < (1.0 / max(lfsr_cluster_rate_hz, 1e-3))).astype(int)\n",
    "                starts = np.where((streak[1:] == 1) & (streak[:-1] == 0))[0] + 1\n",
    "                candB.extend(pd.to_datetime(te.iloc[starts], unit=\"s\").tolist())\n",
    "\n",
    "    # C) long inactivity gaps\n",
    "    tf = f[time_col].astype(\"int64\") / 1e9\n",
    "    dts = np.diff(tf)\n",
    "    candC = f.loc[np.where(dts > min_gap_s)[0], time_col].tolist()\n",
    "\n",
    "    # Sandwich with bounds\n",
    "    bounds = [f[time_col].iloc[0]] + sorted(set(candA + candB + candC)) + [f[time_col].iloc[-1]]\n",
    "    bounds = [t for t in bounds if pd.notna(t)]\n",
    "    if not bounds:\n",
    "        return []\n",
    "    merged = [bounds[0]]\n",
    "    for t in bounds[1:]:\n",
    "        if (t - merged[-1]).total_seconds() > min_gap_s:\n",
    "            merged.append(t)\n",
    "    if len(merged) == 1 and (f[time_col].iloc[-1] > f[time_col].iloc[0]):\n",
    "        merged = [f[time_col].iloc[0], f[time_col].iloc[-1]]\n",
    "    return merged\n",
    "\n",
    "# -------------------------------\n",
    "# Scaled time (fluence-equivalent)\n",
    "# -------------------------------\n",
    "def compute_scaled_time_clipped(\n",
    "    beam_df: pd.DataFrame,\n",
    "    time_col: str = \"time\",\n",
    "    dt_col: str = \"dt\",\n",
    "    flux_col: Optional[str] = \"HEH_dose_rate\",\n",
    "    beam_on_col: Optional[str] = \"beam_on\",\n",
    "    ref: Literal[\"median\", \"mean\", \"max\"] = \"median\",\n",
    "    floor_strategy: Literal[\"adaptive\", \"fixed\"] = \"adaptive\",\n",
    "    min_frac: float = 0.05,\n",
    "    rmax: float = 1e4,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute scaled time to equalize fluence, with floors and caps to prevent explosions.\n",
    "    - When beam_off, we set scaling ratio=1 (i.e., flux=phi_ref).\n",
    "    - Floor flux to max(phi_floor, flux) to avoid huge ratios; cap ratio at rmax.\n",
    "    \"\"\"\n",
    "    b = beam_df.sort_values(time_col).copy()\n",
    "    b[time_col] = to_datetime_smart(b[time_col]).ffill()\n",
    "    dt = pd.to_numeric(b[dt_col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    if (flux_col is not None) and (flux_col in b.columns):\n",
    "        mask_on = (\n",
    "            pd.to_numeric(b.get(beam_on_col, 1), errors=\"coerce\").fillna(0) != 0\n",
    "        )\n",
    "        flux = pd.to_numeric(b[flux_col], errors=\"coerce\")\n",
    "        # reference over beam-on samples\n",
    "        if ref == \"median\":\n",
    "            phi_ref = np.nanmedian(flux[mask_on])\n",
    "        elif ref == \"mean\":\n",
    "            phi_ref = np.nanmean(flux[mask_on])\n",
    "        else:\n",
    "            phi_ref = np.nanmax(flux[mask_on])\n",
    "        if not (np.isfinite(phi_ref) and phi_ref > 0):\n",
    "            phi_ref = 1.0\n",
    "\n",
    "        if floor_strategy == \"fixed\":\n",
    "            phi_floor = min_frac * phi_ref\n",
    "        else:\n",
    "            # adaptive: robust floor from lower tail of on-beam distribution\n",
    "            p10 = np.nanpercentile(flux[mask_on], 10) if mask_on.any() else np.nan\n",
    "            phi_floor = np.nanmax([\n",
    "                0.25 * p10 if np.isfinite(p10) else 0.0,\n",
    "                0.05 * phi_ref,\n",
    "                1e-12,\n",
    "            ])\n",
    "\n",
    "        flux_eff = flux.copy()\n",
    "        flux_eff[~mask_on] = phi_ref  # no scaling when beam is off\n",
    "        flux_eff = np.clip(flux_eff, phi_floor, None)\n",
    "\n",
    "        ratio = (phi_ref / flux_eff).clip(0, rmax)\n",
    "        b[\"dt_eq\"] = dt * ratio\n",
    "    else:\n",
    "        b[\"dt_eq\"] = dt\n",
    "\n",
    "    b[\"t_eq\"] = b[\"dt_eq\"].cumsum()\n",
    "    return b\n",
    "\n",
    "# Legacy simple scaled-time (kept for API compatibility)\n",
    "def compute_scaled_time(\n",
    "    beam_df: pd.DataFrame,\n",
    "    time_col: str = \"time\",\n",
    "    dt_col: str = \"dt\",\n",
    "    flux_col: Optional[str] = \"HEH_dose_rate\",\n",
    "    beam_on_col: Optional[str] = \"beam_on\",\n",
    "    ref: Literal[\"median\", \"mean\", \"max\"] = \"median\",\n",
    ") -> pd.DataFrame:\n",
    "    return compute_scaled_time_clipped(\n",
    "        beam_df, time_col, dt_col, flux_col, beam_on_col, ref,\n",
    "        floor_strategy=\"adaptive\", min_frac=0.05, rmax=1e4,\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# Events from cumulative\n",
    "# -------------------------------\n",
    "def extract_event_times(\n",
    "    fails_df: pd.DataFrame,\n",
    "    time_col: str = \"time\",\n",
    "    cum_col: str = \"failsP_acum\",\n",
    ") -> pd.Series:\n",
    "    f = fails_df.sort_values(time_col).copy()\n",
    "    f[time_col] = to_datetime_smart(f[time_col]).ffill()\n",
    "    c = pd.to_numeric(f[cum_col], errors=\"coerce\").ffill()\n",
    "    inc = c.diff().fillna(0)\n",
    "    return f.loc[inc > 0, time_col]\n",
    "\n",
    "# -------------------------------\n",
    "# Binning strategies\n",
    "# -------------------------------\n",
    "def build_bins_reset_locked(reset_bounds: List[pd.Timestamp], k_multiple: int = 1) -> List[pd.Timestamp]:\n",
    "    if len(reset_bounds) < 2:\n",
    "        return reset_bounds\n",
    "    edges = [reset_bounds[0]]\n",
    "    for i in range(0, len(reset_bounds) - 1, k_multiple):\n",
    "        edges.append(reset_bounds[min(i + k_multiple, len(reset_bounds) - 1)])\n",
    "    out = [edges[0]]\n",
    "    for e in edges[1:]:\n",
    "        if e > out[-1]:\n",
    "            out.append(e)\n",
    "    return out\n",
    "\n",
    "def build_bins_equal_fluence(beam_eq: pd.DataFrame, t_eq_col: str = \"t_eq\", n_bins: int = 20) -> List[pd.Timestamp]:\n",
    "    b = beam_eq.dropna(subset=[t_eq_col]).copy()\n",
    "    if b.empty:\n",
    "        return []\n",
    "    t0, t1 = b[t_eq_col].iloc[0], b[t_eq_col].iloc[-1]\n",
    "    edges_eq = np.linspace(t0, t1, n_bins + 1)\n",
    "    teq = b[t_eq_col].values\n",
    "    treal = to_datetime_smart(b[\"time\"]).astype(\"int64\") / 1e9\n",
    "    real_from_eq = np.interp(edges_eq, teq, treal)\n",
    "    return [pd.to_datetime(x, unit=\"s\") for x in real_from_eq]\n",
    "\n",
    "def build_bins_equal_count(event_times: pd.Series, target_N: int = 25) -> List[pd.Timestamp]:\n",
    "    t = pd.to_datetime(event_times).sort_values().reset_index(drop=True)\n",
    "    if t.empty:\n",
    "        return []\n",
    "    idx = list(range(0, len(t), target_N)) + [len(t) - 1]\n",
    "    edges = [t.iloc[0]] + [t.iloc[i] for i in idx[1:]]\n",
    "    out = [edges[0]]\n",
    "    for e in edges[1:]:\n",
    "        if e > out[-1]:\n",
    "            out.append(e)\n",
    "    return out\n",
    "\n",
    "# -------------------------------\n",
    "# Poisson rate & CIs per bin\n",
    "# -------------------------------\n",
    "@dataclass\n",
    "class BinStat:\n",
    "    t_start: pd.Timestamp\n",
    "    t_end: pd.Timestamp\n",
    "    N: int\n",
    "    T: float\n",
    "    rate: float\n",
    "    lo: float\n",
    "    hi: float\n",
    "\n",
    "\n",
    "def garwood_rate_ci(N: int, T: float, alpha: float = 0.32) -> Tuple[float, float]:\n",
    "    if T <= 0:\n",
    "        return (np.nan, np.nan)\n",
    "    if N == 0:\n",
    "        return (0.0, (-np.log(alpha)) / T)\n",
    "    mu_lo = 0.5 * chi2.ppf(alpha / 2, 2 * N)\n",
    "    mu_hi = 0.5 * chi2.ppf(1 - alpha / 2, 2 * (N + 1))\n",
    "    return (mu_lo / T, mu_hi / T)\n",
    "\n",
    "\n",
    "def summarize_bins(\n",
    "    event_times: pd.Series,\n",
    "    bin_edges: List[pd.Timestamp],\n",
    "    timebase_df: Optional[pd.DataFrame] = None,\n",
    "    use_scaled_time: bool = False,\n",
    "    T_source: Literal[\"beam\", \"wall\"] = \"beam\",\n",
    "    alpha: float = 0.32,\n",
    ") -> List[BinStat]:\n",
    "    \"\"\"Count events in each bin and compute Poisson rate with exact CI.\n",
    "    - T_source=\"beam\": use sum(dt or dt_eq) from timebase_df\n",
    "    - T_source=\"wall\": use wall-clock seconds (ignores timebase df)\n",
    "    \"\"\"\n",
    "    if len(bin_edges) < 2:\n",
    "        return []\n",
    "    t = pd.to_datetime(event_times).sort_values().values\n",
    "    stats: List[BinStat] = []\n",
    "    for t0, t1 in zip(bin_edges[:-1], bin_edges[1:]):\n",
    "        N = int(((t >= np.datetime64(t0)) & (t < np.datetime64(t1))).sum())\n",
    "        if (timebase_df is None) or (T_source == \"wall\" and not use_scaled_time):\n",
    "            T = (t1 - t0).total_seconds()\n",
    "        else:\n",
    "            b = timebase_df\n",
    "            b_time = to_datetime_smart(b[\"time\"])  # type: ignore[index]\n",
    "            m = (b_time >= t0) & (b_time < t1)\n",
    "            col = \"dt_eq\" if use_scaled_time else \"dt\"\n",
    "            T = (\n",
    "                pd.to_numeric(b.loc[m, col], errors=\"coerce\").fillna(0).sum()\n",
    "            )\n",
    "        rate = N / T if T > 0 else np.nan\n",
    "        lo, hi = garwood_rate_ci(N, T, alpha=alpha)\n",
    "        stats.append(BinStat(t0, t1, N, T, rate, lo, hi))\n",
    "    return stats\n",
    "\n",
    "# -------------------------------\n",
    "# Consolidation / orchestration\n",
    "# -------------------------------\n",
    "def build_and_summarize(\n",
    "    df_beam: pd.DataFrame,\n",
    "    fails_df: pd.DataFrame,\n",
    "    *,\n",
    "    bin_mode: Literal[\"reset\", \"fluence\", \"count\"] = \"reset\",\n",
    "    k_multiple: int = 1,\n",
    "    n_bins: int = 20,\n",
    "    target_N: int = 25,\n",
    "    flux_col: Optional[str] = \"HEH_dose_rate\",\n",
    "    area_norm: Optional[Tuple[int, int]] = None,\n",
    "    alpha: float = 0.32,\n",
    "    T_source: Literal[\"beam\", \"wall\"] = \"beam\",\n",
    "    scaled_time_fn = compute_scaled_time,  # can swap for compute_scaled_time_clipped\n",
    "    min_events_per_bin: Optional[int] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"High-level helper that returns a tidy per-bin DataFrame.\n",
    "    - bin_mode: choose binning strategy\n",
    "    - area_norm: (A_run, A_ref) to scale rates for fair cross-run comparison\n",
    "    - T_source: \"beam\" or \"wall\" denominator\n",
    "    - scaled_time_fn: inject different scaled-time policies\n",
    "    - min_events_per_bin: if set, merges adjacent sparse bins until N>=threshold\n",
    "    \"\"\"\n",
    "    beam_eq = scaled_time_fn(df_beam, flux_col=flux_col)\n",
    "    events = extract_event_times(fails_df)\n",
    "\n",
    "    if bin_mode == \"reset\":\n",
    "        resets = detect_resets(fails_df)\n",
    "        edges = build_bins_reset_locked(resets, k_multiple=k_multiple)\n",
    "        use_scaled = False\n",
    "    elif bin_mode == \"fluence\":\n",
    "        edges = build_bins_equal_fluence(beam_eq, n_bins=n_bins)\n",
    "        use_scaled = True\n",
    "    else:\n",
    "        edges = build_bins_equal_count(events, target_N=target_N)\n",
    "        use_scaled = False\n",
    "\n",
    "    if len(edges) < 2:\n",
    "        raise ValueError(\n",
    "            f\"No bins created (len(edges)={len(edges)}). Check time parsing / reset detection. \"\n",
    "            f\"First/last: fails=[{fails_df['time'].iloc[0]} .. {fails_df['time'].iloc[-1]}], \"\n",
    "            f\"beam=[{df_beam['time'].iloc[0]} .. {df_beam['time'].iloc[-1]}]\"\n",
    "        )\n",
    "\n",
    "    stats = summarize_bins(\n",
    "        events, edges, timebase_df=beam_eq, use_scaled_time=use_scaled,\n",
    "        T_source=T_source, alpha=alpha,\n",
    "    )\n",
    "    df_out = pd.DataFrame([s.__dict__ for s in stats])\n",
    "\n",
    "    # enrich\n",
    "    df_out[\"t_mid\"] = df_out[\"t_start\"] + (df_out[\"t_end\"] - df_out[\"t_start\"]) / 2\n",
    "    df_out[\"width_s\"] = (df_out[\"t_end\"] - df_out[\"t_start\"]).dt.total_seconds()\n",
    "\n",
    "    # area normalization\n",
    "    if (area_norm is not None) and (len(df_out) > 0):\n",
    "        A_run, A_ref = area_norm\n",
    "        scale = (A_ref / A_run) if (A_run and A_ref) else 1.0\n",
    "        for col in [\"rate\", \"lo\", \"hi\"]:\n",
    "            df_out[col] = df_out[col] * scale\n",
    "\n",
    "    # optional: merge sparse bins to guarantee minimum events per bin\n",
    "    if (min_events_per_bin is not None) and (min_events_per_bin > 0):\n",
    "        df_out = merge_sparse_bins(df_out, k_min=min_events_per_bin)\n",
    "\n",
    "    return df_out\n",
    "\n",
    "# -------------------------------\n",
    "# QA / Utilities\n",
    "# -------------------------------\n",
    "def inspect_scaled_time(beam_eq: pd.DataFrame, label: str, clip_warn_ratio: float = 1e2) -> None:\n",
    "    dt = pd.to_numeric(beam_eq.get(\"dt\", 0), errors=\"coerce\").fillna(0)\n",
    "    dt_eq = pd.to_numeric(beam_eq.get(\"dt_eq\", 0), errors=\"coerce\").fillna(0)\n",
    "    ratio = (dt_eq / dt).replace([np.inf, -np.inf], np.nan)\n",
    "    ratio = ratio[dt > 0]\n",
    "    if len(ratio) == 0:\n",
    "        print(f\"[SCALED TIME] {label}: no valid dt rows.\")\n",
    "        return\n",
    "    q50, q90, q99, mx = np.nanpercentile(ratio, [50, 90, 99, 100])\n",
    "    print(f\"[SCALED TIME] {label}: median={q50:.3g}, p90={q90:.3g}, p99={q99:.3g}, max={mx:.3g}\")\n",
    "    if mx > clip_warn_ratio:\n",
    "        print(f\"⚠️  Large scaling detected (max {mx:.1f}×). Consider tighter floor/cap or gating by beam_on.\")\n",
    "\n",
    "\n",
    "def merge_sparse_bins(df_stats: pd.DataFrame, k_min: int = 5) -> pd.DataFrame:\n",
    "    rows = df_stats.sort_values(\"t_start\").to_dict(\"records\")\n",
    "    out: List[dict] = []\n",
    "    acc = None\n",
    "    for r in rows:\n",
    "        if acc is None:\n",
    "            acc = r\n",
    "            continue\n",
    "        if acc[\"N\"] < k_min:\n",
    "            # merge acc + r\n",
    "            acc[\"t_end\"] = r[\"t_end\"]\n",
    "            acc[\"N\"] += r[\"N\"]\n",
    "            acc[\"T\"] += r[\"T\"]\n",
    "            acc[\"rate\"] = acc[\"N\"] / acc[\"T\"] if acc[\"T\"] > 0 else np.nan\n",
    "            acc[\"lo\"], acc[\"hi\"] = garwood_rate_ci(acc[\"N\"], acc[\"T\"], alpha=0.32)\n",
    "            acc[\"t_mid\"] = acc[\"t_start\"] + (acc[\"t_end\"] - acc[\"t_start\"]) / 2\n",
    "            acc[\"width_s\"] = (acc[\"t_end\"] - acc[\"t_start\"]).total_seconds()\n",
    "        else:\n",
    "            out.append(acc)\n",
    "            acc = r\n",
    "    if acc is not None:\n",
    "        out.append(acc)\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "\n",
    "def conservation_checks(events: pd.Series, edges: List[pd.Timestamp], beam_eq: pd.DataFrame, use_scaled: bool) -> None:\n",
    "    t = pd.to_datetime(events).sort_values().values\n",
    "    Ns, Ts = [], []\n",
    "    for t0, t1 in zip(edges[:-1], edges[1:]):\n",
    "        Ns.append(int(((t >= np.datetime64(t0)) & (t < np.datetime64(t1))).sum()))\n",
    "        b_time = to_datetime_smart(beam_eq[\"time\"])  # type: ignore[index]\n",
    "        m = (b_time >= t0) & (b_time < t1)\n",
    "        Ts.append(pd.to_numeric(beam_eq.loc[m, \"dt_eq\" if use_scaled else \"dt\"], errors=\"coerce\").fillna(0).sum())\n",
    "    print(f\"Events total: {len(t)} | Sum of bin N: {sum(Ns)}\")\n",
    "    print(f\"Sum(T) over bins: {sum(Ts):.3f}\")\n",
    "\n",
    "\n",
    "def check_real_output(df_out: pd.DataFrame, name: str, use_scaled: bool = False, tol_ratio: float = 0.20) -> None:\n",
    "    print(f\"\\n[REAL CHECK] {name}\")\n",
    "    req = {\"t_start\",\"t_end\",\"N\",\"T\",\"rate\",\"lo\",\"hi\",\"t_mid\",\"width_s\"}\n",
    "    missing = req - set(df_out.columns)\n",
    "    assert not missing, f\"Missing columns: {missing}\"\n",
    "    assert (df_out[\"t_end\"] > df_out[\"t_start\"]).all(), \"Found non-positive bin widths.\"\n",
    "    assert (df_out[\"width_s\"] > 0).all(), \"Non-positive width_s.\"\n",
    "    bad_ci = df_out[(df_out[\"T\"] > 0) & ~((df_out[\"lo\"] <= df_out[\"rate\"]) & (df_out[\"rate\"] <= df_out[\"hi\"]))]\n",
    "    assert bad_ci.empty, f\"Rate outside CI in {len(bad_ci)} bins.\"\n",
    "    if not use_scaled and len(df_out) > 0:\n",
    "        S_T = df_out[\"T\"].sum()\n",
    "        S_W = df_out[\"width_s\"].sum()\n",
    "        rel = abs(S_T - S_W) / max(S_W, 1e-9)\n",
    "        print(f\"sum(T)={S_T:.3f}, sum(width_s)={S_W:.3f}, rel diff={rel:.3%}\")\n",
    "        assert rel <= tol_ratio, f\"sum(T) differs from sum(width_s) by {rel:.1%} (> {tol_ratio:.0%}).\"\n",
    "\n",
    "\n",
    "def target_N_for_relative_error(RE: float = 0.2) -> int:\n",
    "    return int(np.ceil((1.0 / max(RE, 1e-9)) ** 2))\n",
    "\n",
    "# Optional GLM (requires statsmodels). Kept safe with local import.\n",
    "def poisson_trend_test(df_stats: pd.DataFrame, x_col: str = \"t_mid\") -> dict:\n",
    "    try:\n",
    "        import statsmodels.api as sm  # type: ignore\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"statsmodels is required for poisson_trend_test().\") from e\n",
    "    x = (\n",
    "        (pd.to_datetime(df_stats[x_col]) - pd.to_datetime(df_stats[x_col]).min())\n",
    "        .dt.total_seconds()\n",
    "        .values\n",
    "        / 3600.0\n",
    "    )\n",
    "    y = df_stats[\"N\"].values\n",
    "    T = df_stats[\"T\"].values\n",
    "    X = sm.add_constant(x)\n",
    "    model = sm.GLM(y, X, family=sm.families.Poisson(), offset=np.log(np.clip(T, 1e-12, None)))\n",
    "    res = model.fit()\n",
    "    return {\n",
    "        \"slope_per_hour\": float(res.params[1]),\n",
    "        \"p_value\": float(res.pvalues[1]),\n",
    "        \"AIC\": float(res.aic),\n",
    "        \"summary\": res.summary().as_text(),\n",
    "    }\n",
    "\n",
    "__all__ = [\n",
    "    \"to_datetime_smart\",\n",
    "    \"detect_resets\",\n",
    "    \"compute_scaled_time\",\n",
    "    \"compute_scaled_time_clipped\",\n",
    "    \"extract_event_times\",\n",
    "    \"build_bins_reset_locked\",\n",
    "    \"build_bins_equal_fluence\",\n",
    "    \"build_bins_equal_count\",\n",
    "    \"BinStat\",\n",
    "    \"garwood_rate_ci\",\n",
    "    \"summarize_bins\",\n",
    "    \"build_and_summarize\",\n",
    "    \"inspect_scaled_time\",\n",
    "    \"merge_sparse_bins\",\n",
    "    \"conservation_checks\",\n",
    "    \"check_real_output\",\n",
    "    \"target_N_for_relative_error\",\n",
    "    \"poisson_trend_test\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff9c55",
   "metadata": {},
   "source": [
    "## checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a655f43",
   "metadata": {},
   "source": [
    "Run on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b7dbad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCALED TIME] run2 HEH (clipped): median=1, p90=1.78, p99=6.24, max=8.7\n",
      "\n",
      "[REAL CHECK] run2 fluence-binned (clipped)\n"
     ]
    }
   ],
   "source": [
    "# 1) Import or paste the v2 module code from the canvas\n",
    "\n",
    "# 2) Build scaled time with clipping, then run fluence binning\n",
    "beam_eq2c = compute_scaled_time_clipped(df_beam2, flux_col=\"HEH_dose_rate\",\n",
    "                                        floor_strategy=\"adaptive\", rmax=1e4)\n",
    "inspect_scaled_time(beam_eq2c, \"run2 HEH (clipped)\")\n",
    "\n",
    "res_run2_flu = build_and_summarize(\n",
    "    df_beam=df_beam2, fails_df=fails_run2,\n",
    "    bin_mode=\"fluence\", n_bins=30,\n",
    "    flux_col=\"HEH_dose_rate\",\n",
    "    area_norm=(32,32),\n",
    "    alpha=0.32,\n",
    "    T_source=\"beam\",                     # or \"wall\"\n",
    "    scaled_time_fn=compute_scaled_time_clipped,\n",
    "    min_events_per_bin=5,                # optional stability\n",
    ")\n",
    "\n",
    "check_real_output(res_run2_flu, \"run2 fluence-binned (clipped)\", use_scaled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990af7c",
   "metadata": {},
   "source": [
    "median=1 → most intervals had flux ≈ reference flux (no scaling).\n",
    "\n",
    "p90=1.78 → top 10% of intervals had ~1.8× lower flux than reference.\n",
    "\n",
    "p99=6.24, max=8.7 → worst intervals scale only ~6–9×. That’s orders-of-magnitude better than before (no Δt_eq explosions). Your floor+cap is doing its job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c63db795",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_for_10pct = target_N_for_relative_error(0.10)  # -> 100\n",
    "res_run2_eqN = build_and_summarize(\n",
    "    df_beam2, fails_run2, bin_mode=\"count\", target_N=N_for_10pct,\n",
    "    flux_col=\"HEH_dose_rate\", area_norm=(32,32)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7a78b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_start</th>\n",
       "      <th>t_end</th>\n",
       "      <th>N</th>\n",
       "      <th>T</th>\n",
       "      <th>rate</th>\n",
       "      <th>lo</th>\n",
       "      <th>hi</th>\n",
       "      <th>t_mid</th>\n",
       "      <th>width_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-15 10:48:48.721228</td>\n",
       "      <td>2022-09-15 11:25:24.986096</td>\n",
       "      <td>100</td>\n",
       "      <td>2203.416392</td>\n",
       "      <td>0.045384</td>\n",
       "      <td>0.040877</td>\n",
       "      <td>0.050365</td>\n",
       "      <td>2022-09-15 11:07:06.853662000</td>\n",
       "      <td>2196.264868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-15 11:25:24.986096</td>\n",
       "      <td>2022-09-15 11:28:14.920450</td>\n",
       "      <td>100</td>\n",
       "      <td>165.256229</td>\n",
       "      <td>0.605121</td>\n",
       "      <td>0.545023</td>\n",
       "      <td>0.671527</td>\n",
       "      <td>2022-09-15 11:26:49.953273000</td>\n",
       "      <td>169.934354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-15 11:28:14.920450</td>\n",
       "      <td>2022-09-15 11:31:32.822763</td>\n",
       "      <td>100</td>\n",
       "      <td>220.341639</td>\n",
       "      <td>0.453841</td>\n",
       "      <td>0.408767</td>\n",
       "      <td>0.503645</td>\n",
       "      <td>2022-09-15 11:29:53.871606500</td>\n",
       "      <td>197.902313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-15 11:31:32.822763</td>\n",
       "      <td>2022-09-15 15:00:12.521610</td>\n",
       "      <td>100</td>\n",
       "      <td>12504.388024</td>\n",
       "      <td>0.007997</td>\n",
       "      <td>0.007203</td>\n",
       "      <td>0.008875</td>\n",
       "      <td>2022-09-15 13:15:52.672186500</td>\n",
       "      <td>12519.698847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-15 15:00:12.521610</td>\n",
       "      <td>2022-09-15 15:02:43.820547</td>\n",
       "      <td>100</td>\n",
       "      <td>165.256229</td>\n",
       "      <td>0.605121</td>\n",
       "      <td>0.545023</td>\n",
       "      <td>0.671527</td>\n",
       "      <td>2022-09-15 15:01:28.171078500</td>\n",
       "      <td>151.298937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     t_start                      t_end    N             T  \\\n",
       "0 2022-09-15 10:48:48.721228 2022-09-15 11:25:24.986096  100   2203.416392   \n",
       "1 2022-09-15 11:25:24.986096 2022-09-15 11:28:14.920450  100    165.256229   \n",
       "2 2022-09-15 11:28:14.920450 2022-09-15 11:31:32.822763  100    220.341639   \n",
       "3 2022-09-15 11:31:32.822763 2022-09-15 15:00:12.521610  100  12504.388024   \n",
       "4 2022-09-15 15:00:12.521610 2022-09-15 15:02:43.820547  100    165.256229   \n",
       "\n",
       "       rate        lo        hi                         t_mid       width_s  \n",
       "0  0.045384  0.040877  0.050365 2022-09-15 11:07:06.853662000   2196.264868  \n",
       "1  0.605121  0.545023  0.671527 2022-09-15 11:26:49.953273000    169.934354  \n",
       "2  0.453841  0.408767  0.503645 2022-09-15 11:29:53.871606500    197.902313  \n",
       "3  0.007997  0.007203  0.008875 2022-09-15 13:15:52.672186500  12519.698847  \n",
       "4  0.605121  0.545023  0.671527 2022-09-15 15:01:28.171078500    151.298937  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_run2_eqN.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
