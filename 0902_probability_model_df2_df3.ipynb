{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee4a2d0",
   "metadata": {},
   "source": [
    "\n",
    "# 0902 - Initial Probability Model Kickoff (df2/df3 small runs)\n",
    "\n",
    "This notebook stages the smaller validation extracts (`df2_valid_small.csv`, `df3_valid_small.csv`) for the Poisson bathtub workflow described in [docs/initial_probability_models.md](docs/initial_probability_models.md). It demonstrates how the refreshed library helpers guide analysts from raw beam counters to fluence-binned failure rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e930a",
   "metadata": {},
   "source": [
    "\n",
    "## Objectives\n",
    "\n",
    "* Load the validation subsets and beam monitor data using the documented helper APIs.\n",
    "* Gate failure events by beam-on exposure and compute fluence-equivalent time with `radbin.core.compute_scaled_time_clipped`.\n",
    "* Build first-pass fluence bins with `radbin.core.build_and_summarize`, store the summary, and visualise the resulting sigma_SEE profile.\n",
    "* Run `radbin.glm.poisson_trend_test_plus` as the near-term SMART goal to identify any monotonic drift in the failure rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aec47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from lib.beam import read_beam_data, beam_pipeline\n",
    "from lib.cpld_events import detect_bit_increments\n",
    "from radbin.core import (\n",
    "    compute_scaled_time_clipped,\n",
    "    inspect_scaled_time,\n",
    "    build_and_summarize,\n",
    "    plot_cumulative_fails,\n",
    "    to_datetime_smart,\n",
    ")\n",
    "from radbin.glm import poisson_trend_test_plus, format_trend_report\n",
    "from radbin.plots import bar_rates, plot_scaling_ratio\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = Path('1_data')\n",
    "FAILURE_FILES = [\n",
    "    DATA_DIR / 'df2_valid_small.csv',\n",
    "    DATA_DIR / 'df3_valid_small.csv',\n",
    "]\n",
    "# Adjust if the beam extract uses a different filename locally.\n",
    "BEAM_FILE = DATA_DIR / 'beam3.csv'\n",
    "\n",
    "RESULTS_DIR = Path('results') / 'radbin'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_CSV = RESULTS_DIR / 'run_df2_df3_fluence.csv'\n",
    "\n",
    "print(f\"Beam file: {BEAM_FILE}\")\n",
    "print('Failure logs:')\n",
    "for path in FAILURE_FILES:\n",
    "    print(f'  - {path}')\n",
    "print(f\"Fluence-bin output -> {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a085b64",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load and inspect the beam monitor\n",
    "\n",
    "The `lib.beam.read_beam_data` helper enforces the column naming conventions adopted in the historical notebooks (`time`, `TID`, `HEH`, `N1MeV`) and flags any non-monotonic counter segments before we proceed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d478544",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beam_df = read_beam_data(BEAM_FILE, run_id=32, plot=True, title='Beam monitor check - df2/df3 subset')\n",
    "print(f\"Loaded {len(beam_df):,} beam rows spanning {beam_df['time'].min()} -> {beam_df['time'].max()}\")\n",
    "beam_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ba898",
   "metadata": {},
   "source": [
    "\n",
    "### Differential dose rates\n",
    "\n",
    "`lib.beam.beam_pipeline` computes per-sample dose rates and a boolean `beam_on` flag (defaults to `TID_dose_rate > 1e-7 Gy/s`). This mirrors the staging performed in the Poisson bathtub notebooks before bin construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc73227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beam_processed = beam_pipeline(beam_df, epsilon=1e-7, debug=True)\n",
    "beam_processed.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5792fa14",
   "metadata": {},
   "source": [
    "\n",
    "### Fluence-equivalent timebase\n",
    "\n",
    "The probability models use fluence-scaled exposure (`dt_eq`, `t_eq`) rather than wall-clock durations. `radbin.core.compute_scaled_time_clipped` reproduces the notebook logic, including beam-off freezing and adaptive floors. The diagnostic call highlights any extreme scaling ratios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ef63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beam_eq = compute_scaled_time_clipped(\n",
    "    beam_processed,\n",
    "    flux_col='HEH_dose_rate',\n",
    "    beam_on_col='beam_on',\n",
    "    mode='fluence',\n",
    ")\n",
    "inspect_scaled_time(beam_eq, label='df2/df3 fluence track')\n",
    "fluence_total = beam_eq.loc[beam_eq['beam_on'] == 1, 'dt_eq'].sum()\n",
    "print(f'Total integrated fluence (beam-on): {fluence_total:,.3e}')\n",
    "beam_eq[['time', 'dt', 'dt_eq', 't_eq', 'beam_on']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00528ca",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Load the CPLD failure logs\n",
    "\n",
    "The helper below normalises each CSV so that downstream functions receive a monotonic cumulative counter (`failsP_acum`). It auto-detects timestamp columns and, when needed, rebuilds the cumulative sequence from per-event increments or per-bit counters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f6e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_failure_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Return a tidy failure counter table ready for RadBIN utilities.\"\"\"\n",
    "    df_raw = pd.read_csv(path)\n",
    "    candidate_time_cols = [col for col in ['time', 'timestamp', 'event_time', 't', 'datetime'] if col in df_raw.columns]\n",
    "    if not candidate_time_cols:\n",
    "        raise KeyError(f'No timestamp column found in {path}. Available columns: {df_raw.columns.tolist()}')\n",
    "    time_col = candidate_time_cols[0]\n",
    "    df_raw['time'] = to_datetime_smart(df_raw[time_col])\n",
    "    df = df_raw.dropna(subset=['time']).sort_values('time').reset_index(drop=True)\n",
    "\n",
    "    if 'failsP_acum' in df.columns:\n",
    "        df['failsP_acum'] = pd.to_numeric(df['failsP_acum'], errors='coerce').ffill().fillna(0).astype(int)\n",
    "    elif 'total_fails' in df.columns:\n",
    "        df['failsP_acum'] = pd.to_numeric(df['total_fails'], errors='coerce').ffill().fillna(0).astype(int)\n",
    "    elif 'increment' in df.columns:\n",
    "        increments = pd.to_numeric(df['increment'], errors='coerce').fillna(0).astype(int)\n",
    "        df['failsP_acum'] = increments.cumsum().astype(int)\n",
    "    else:\n",
    "        bit_cols = [c for c in df.columns if c.startswith('bitn') and c[len('bitn'):].isdigit()]\n",
    "        if bit_cols:\n",
    "            counters = df[['time'] + bit_cols].copy()\n",
    "            events = detect_bit_increments(counters, time_col='time')\n",
    "            if events.empty:\n",
    "                df['failsP_acum'] = 0\n",
    "            else:\n",
    "                increments = events.groupby('time')['increment'].sum().astype(int)\n",
    "                df = df.merge(increments.rename('increment'), on='time', how='left')\n",
    "                df['increment'] = df['increment'].fillna(0).astype(int)\n",
    "                df['failsP_acum'] = df['increment'].cumsum().astype(int)\n",
    "        else:\n",
    "            raise KeyError(f'Could not infer cumulative failures for {path}.')\n",
    "\n",
    "    if 'increment' not in df.columns:\n",
    "        df['increment'] = df['failsP_acum'].diff().fillna(df['failsP_acum']).clip(lower=0).astype(int)\n",
    "\n",
    "    df = (\n",
    "        df.groupby('time', as_index=False)\n",
    "          .agg(failsP_acum=('failsP_acum', 'max'), increment=('increment', 'sum'))\n",
    "          .sort_values('time')\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    df['failsP_acum'] = df['failsP_acum'].cummax().astype(int)\n",
    "    df['increment'] = df['increment'].clip(lower=0).astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "failure_tables = []\n",
    "for path in FAILURE_FILES:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f'Expected failure log {path} was not found. Update FAILURE_FILES if necessary.')\n",
    "    table = load_failure_csv(path)\n",
    "    print(f\"{path.name}: {len(table):,} rows, {table['failsP_acum'].iloc[-1]:,} cumulative fails\")\n",
    "    failure_tables.append(table)\n",
    "\n",
    "failures = pd.concat(failure_tables, ignore_index=True).sort_values('time').reset_index(drop=True)\n",
    "failures['failsP_acum'] = failures['failsP_acum'].cummax().astype(int)\n",
    "failures['increment'] = failures['failsP_acum'].diff().fillna(failures['failsP_acum']).clip(lower=0).astype(int)\n",
    "print(f\"Combined: {len(failures):,} rows / {failures['failsP_acum'].iloc[-1]:,} total events\")\n",
    "failures.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b0913",
   "metadata": {},
   "source": [
    "\n",
    "### Quick visual check\n",
    "\n",
    "Plotting the cumulative counter ensures we inherited the expected bathtub outline before merging with the beam stream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dabc61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_fails(failures, title='Cumulative CPLD failures - df2/df3 subset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ccb591",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Gate failures by beam exposure\n",
    "\n",
    "We align the failure timestamps with the processed beam table to preserve only the events observed while the beam was active. The `merge_asof` call mirrors the historical notebooks and attaches the instantaneous dose rate for additional QA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aligned = pd.merge_asof(\n",
    "    failures.sort_values('time'),\n",
    "    beam_processed[['time', 'beam_on', 'HEH_dose_rate', 'dt']],\n",
    "    on='time',\n",
    "    direction='backward'\n",
    ")\n",
    "aligned['beam_on'] = aligned['beam_on'].fillna(False)\n",
    "failures_on = aligned[aligned['beam_on']].copy()\n",
    "print(f\"Beam-gated failures: {len(failures_on):,} / {len(failures):,} rows remain\")\n",
    "failures_on.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f935ed6",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Fluence binning and sigma_SEE profile\n",
    "\n",
    "We reuse the binning strategy from the 0829 notebook: fluence-equal bins, Garwood confidence intervals, minimum exposure and event guardrails, and the standard `(area_run=32, area_ref=1)` normalisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d918213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_BINS = 12\n",
    "T_min = 0.2 * fluence_total / N_BINS if N_BINS > 0 else None\n",
    "if T_min:\n",
    "    print('Target bins: {}, min exposure per bin: {:.3e}'.format(N_BINS, T_min))\n",
    "else:\n",
    "    print(f'Target bins: {N_BINS}, min exposure per bin: not set')\n",
    "\n",
    "bin_summary = build_and_summarize(\n",
    "    df_beam=beam_processed,\n",
    "    fails_df=failures_on,\n",
    "    bin_mode='fluence',\n",
    "    n_bins=N_BINS,\n",
    "    flux_col='HEH_dose_rate',\n",
    "    area_norm=(32, 1),\n",
    "    alpha=0.35,\n",
    "    min_events_per_bin=5,\n",
    "    min_exposure_per_bin=T_min,\n",
    ")\n",
    "if bin_summary.empty:\n",
    "    raise RuntimeError('No fluence bins were produced; check the beam gating and cumulative counters.')\n",
    "\n",
    "bin_summary['sigma_SEE'] = bin_summary['N'] / bin_summary['T']\n",
    "print('Binning produced {} bins with median sigma_SEE = {:.3e}'.format(len(bin_summary), bin_summary['sigma_SEE'].median()))\n",
    "bin_summary.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de5c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_summary.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f'Saved fluence summary to {OUTPUT_CSV}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a057e",
   "metadata": {},
   "source": [
    "\n",
    "### Visual diagnostics\n",
    "\n",
    "* `bar_rates`: log-scale sigma_SEE bars with Garwood error bars.\n",
    "* `plot_scaling_ratio`: sanity-check the exposure scaling used by `compute_scaled_time_clipped`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = bar_rates(\n",
    "    bin_summary,\n",
    "    y_var='rate',\n",
    "    title='sigma_SEE per fluence bin - df2/df3 subset',\n",
    "    logy=True,\n",
    ")\n",
    "_ = ax.figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348d5d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax = plot_scaling_ratio(beam_eq, label='df2/df3 fluence scaling')\n",
    "_ = ax.figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd642ea3",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Poisson trend test\n",
    "\n",
    "With the bins validated, we execute the SMART-goal GLM to quantify monotonic drift. The robust standard errors handle mild over-dispersion, and the formatted report matches the agreed template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trend = poisson_trend_test_plus(\n",
    "    bin_summary,\n",
    "    count='N',\n",
    "    exposure='T',\n",
    "    time_col='t_mid',\n",
    "    alpha=0.05,\n",
    "    se_method='robust',\n",
    "    use_lrt=True,\n",
    "    equivalence_rr=1.02,  # treat +/- 2 %/hour as practically equivalent\n",
    ")\n",
    "print(format_trend_report(trend, time_unit='fluence-bin', alpha=0.05))\n",
    "trend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6974c2",
   "metadata": {},
   "source": [
    "\n",
    "## Next actions\n",
    "\n",
    "1. Replicate the binning for the remaining runs or larger extracts to cross-validate the sigma_SEE envelope.\n",
    "2. Feed `bin_summary` into the Bayesian bathtub prototype (piecewise-constant hazards seeded by the Garwood intervals).\n",
    "3. Integrate the trend-test result into the operations note, highlighting the statistical significance and practical-equivalence verdict.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
